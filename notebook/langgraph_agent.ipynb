{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from config import Config\n",
    "config = Config.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 15:19:16,436 - utils.prompt_loader - INFO - Preloaded prompt: search.j2\n",
      "2025-05-28 15:19:16,441 - utils.prompt_loader - INFO - Preloaded prompt: summarise_news.j2\n"
     ]
    }
   ],
   "source": [
    "from utils import prompt_loader\n",
    "prompt_loader.list_prompts()\n",
    "SEARCH_PROMPT = prompt_loader.get_prompt(\"search\").template.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_model.news import NewsArticleList\n",
    "llm = ChatOpenAI(model=config.openai_search_tools.model)\n",
    "tool = {\"type\": config.openai_search_tools.type, \n",
    "        \"search_context_size\": config.openai_search_tools.search_context_size}\n",
    "llm_with_tools = llm.bind_tools([tool])\n",
    "structured_llm = llm_with_tools.with_structured_output(NewsArticleList)\n",
    "response = structured_llm.invoke(SEARCH_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NewsArticle(title='Meta Releases Chameleon: New Multimodal Model for Text and Image Comprehension', source='Meta AI', date=datetime.date(2024, 6, 18), url='https://ai.meta.com/blog/chameleon-multimodal-ai-model/', summary='Meta has unveiled Chameleon, a family of early-fusion token-based models that jointly process and generate both text and images. Achieving state-of-the-art performance in text-only, image-only, and multimodal benchmarks, Chameleon demonstrates impressive capabilities in image captioning, visual question answering, and reasoning with mixed modalities. The models were trained both from scratch and through large-scale experiments, and Meta has released inference code and model weights for research use.'),\n",
       " NewsArticle(title='Mistral Releases Mistral NeMo: Efficient Open-Weighted Transformer Model', source='Mistral AI', date=datetime.date(2024, 6, 18), url='https://github.com/mistralai/Mistral-NeMo', summary='Mistral has open-sourced Mistral NeMo, a new Transformer-backed model with open weights targeting efficient inference for a variety of tasks. The library provides tools for training, evaluation, and deployment under a highly permissive license, enabling researchers and developers to customize and fine-tune the model for their own applications.'),\n",
       " NewsArticle(title='OpenAI GPT-4o Model Card and System Card Released, Detailing Safety Evaluations and Model Capabilities', source='OpenAI', date=datetime.date(2024, 6, 14), url='https://openai.com/research/gpt-4o-system-card', summary='OpenAI has published the full model card and system card for its multimodal GPT-4o model. The documents provide detailed insights into model architecture, training data sources, evaluation methodologies, and safety strategies, presenting results on robustness, bias, refusal, and multilingual performance, as well as ongoing research directions for the next generation of foundation models.'),\n",
       " NewsArticle(title='Google DeepMind Proposes GNoME: Large-Scale Graph Neural Networks for Accelerated Materials Discovery', source='arXiv (DeepMind)', date=datetime.date(2024, 6, 17), url='https://arxiv.org/abs/2406.09100', summary='DeepMind researchers present GNoME, an advancement of large-scale graph neural networks applied to crystal structure discovery. Leveraging massive datasets and self-correcting training paradigms, GNoME predicts stable structures with unprecedented accuracy. The system has enabled over 380,000 novel stable materials predictions, substantially accelerating computational materials science, with implications for energy, electronics, and AI-accelerated science.'),\n",
       " NewsArticle(title='Databricks Releases DBRX Instruct: Strong New Open-Source LLM for Instruction Following', source='Databricks', date=datetime.date(2024, 6, 15), url='https://databricks.com/blog/dbx-instruct-open-llm', summary='Databricks has open-sourced DBRX Instruct, a new 132B-parameter language model with robust instruction following, reasoning, and code capabilities. Benchmarks show DBRX Instruct outperforming other open LLMs on several popular language understanding and generation tasks. Model weights and training code are available for research and deployment.')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-news",
   "language": "python",
   "name": "ai-news"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
